{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import truncnorm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import datetime\n",
    "import tqdm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def flatten_list(list_array):\n",
    "    return list(itertools.chain(*list_array))\n",
    "\n",
    "sys.path.insert(0,\"../\")\n",
    "from global_config import config\n",
    "\n",
    "results_dir           = config.get_property('results_dir')\n",
    "results2_dir          = config.get_property('results2_dir')\n",
    "data_dir              = config.get_property('data_dir')\n",
    "paper_dir             = config.get_property('paper_dir')\n",
    "data_db_dir           = config.get_property('data_db_dir')\n",
    "feb_hosp_records_path = os.path.join(data_db_dir, 'long_files_8_25_2021')\n",
    "path_to_save          = os.path.join(results_dir, \"real_testing\", \"community\")\n",
    "\n",
    "COLOR_LIST1           = [\"#F8AFA8\", \"#FDDDA0\", \"#F5CDB4\", \"#74A089\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_population_data(path_to_file, date_start=pd.to_datetime(\"2020-02-01\"), date_end=pd.to_datetime(\"2021-02-28\")):\n",
    "\n",
    "    dates_simulation = pd.date_range(start=date_start, end=date_end, freq=\"D\")\n",
    "\n",
    "    data_df  = pd.read_csv( path_to_file, parse_dates=['date'])\n",
    "    data_df  = data_df[data_df.date.isin(dates_simulation)]\n",
    "    A_df     = pd.pivot(data_df, index='ward', columns='date', values='num_admitted')\n",
    "    D_df     = pd.pivot(data_df, index='ward', columns='date', values='num_discharged')\n",
    "    H_df     = pd.pivot(data_df, index='ward', columns='date', values='num_hospitalized')\n",
    "    tests_df = pd.pivot(data_df, index='ward', columns='date', values='num_tested')\n",
    "    Hmean_df = H_df.mean(axis=1)\n",
    "\n",
    "    return A_df, D_df, H_df, tests_df, Hmean_df\n",
    "\n",
    "def create_time_transfers(path_to_file, num_wards, ward_names, date_start=pd.to_datetime(\"2020-02-01\"), date_end=pd.to_datetime(\"2021-02-28\")):\n",
    "\n",
    "    dates_simulation = pd.date_range(start=date_start, end=date_end, freq=\"D\")\n",
    "    transfers_df     = pd.read_csv(path_to_file, parse_dates=['date'])\n",
    "    transfers_df     = transfers_df[transfers_df.date.isin(dates_simulation)]\n",
    "    M_df             = np.zeros((num_wards, num_wards, len(dates_simulation)+1))\n",
    "\n",
    "    for i in range(num_wards):\n",
    "        ward_from = ward_names[i]\n",
    "        for j in range(num_wards):\n",
    "            ward_to      = ward_names[j]\n",
    "            transfers_ij = transfers_df[(transfers_df.ward_from==ward_from) & (transfers_df.ward_to==ward_to)]\n",
    "\n",
    "            if(transfers_ij.shape[0] > 0) :\n",
    "                dates_ij                = transfers_ij.date.values\n",
    "                dates_ind               = np.where(np.in1d(dates_ij, dates_simulation))[0]\n",
    "                transfered              = transfers_ij.num_transfered.values\n",
    "                M_df[i, j, dates_ind-1] = transfered\n",
    "\n",
    "    return M_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ward_counts = os.path.join(data_db_dir, \"long_files_8_25_2021\", \"counts_ward.csv\" )\n",
    "path_to_ward_transf = os.path.join(data_db_dir, \"long_files_8_25_2021\", \"transfers_ward.csv\" )\n",
    "\n",
    "dates_simulation    = pd.date_range(start=pd.to_datetime(\"2020-02-01\"), end=pd.to_datetime(\"2021-02-28\"), freq=\"D\")\n",
    "\n",
    "A_df, D_df, H_df, tests_df, pop = create_population_data(path_to_ward_counts)\n",
    "\n",
    "num_pop    = len(pop)\n",
    "ward_names = list(pop.index)\n",
    "M_df       = create_time_transfers(path_to_ward_transf, num_wards=num_pop, ward_names=ward_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import process_metapop, observe_metapop, init_metapop, simulate_metapop, simulate_metapop_observations\n",
    "\n",
    "if_settings = {\n",
    "   \"Nif\"                : 50,          # number of iterations of the IF\n",
    "   \"type_cooling\"       : \"geometric\", # type of cooling schedule\n",
    "   \"shrinkage_factor\"   : 0.9,         # shrinkage factor for the cooling schedule\n",
    "   \"inflation\"          : 1.01,        # inflation factor for spreading the variance after the EAKF step\n",
    "}\n",
    "\n",
    "model_settings = {\n",
    "    \"param_name\"  : [\"ρ\", \"β\"],            # importation and transmission rate\n",
    "    \"p\"           : 2,                     # number of parameters\n",
    "    \"k\"           : num_pop,               # number of observations | We are just observing carriage\n",
    "    \"n\"           : 3*num_pop,             # number of state variables / dimension of the state space\n",
    "    \"dt\"          : 1,                     # time step\n",
    "    \"T\"           : len(dates_simulation), # time to run\n",
    "    \"m\"           : 100,                   # number of ensembles\n",
    "    \"stochastic\"  : True,                  # is stochastic\n",
    "    \"num_pop\"     : num_pop,\n",
    "    \"dates\"       : dates_simulation\n",
    "    }\n",
    "\n",
    "p = model_settings[\"p\"]\n",
    "m = model_settings[\"m\"]\n",
    "T = model_settings[\"T\"]\n",
    "\n",
    "delta = 1/120  # decolonization rate\n",
    "\n",
    "A     = A_df.to_numpy()\n",
    "D     = D_df.to_numpy()\n",
    "H     = H_df.to_numpy()\n",
    "M     = M_df\n",
    "tests = tests_df.to_numpy()\n",
    "\n",
    "# Process model for the ifeakf | model(x, gamma, beta, delta, rho, sigma, pop, m=1, stochastic=True)\n",
    "process_model_gamma = lambda t, x, θ, gamma : process_metapop(t, x,\n",
    "                                            gamma = gamma * np.ones(m),\n",
    "                                            beta  = θ[1, :],\n",
    "                                            delta = delta,\n",
    "                                            Nmean = np.expand_dims(pop, -1),\n",
    "                                            N     = H[:, [t]],\n",
    "                                            A     = A[:, [t]],\n",
    "                                            D     = D[:, [t]],\n",
    "                                            M     = M[:, :, t])\n",
    "\n",
    "# Observational model for the ifeakf |  g(t, x, rho)\n",
    "observational_model  = lambda t, x, θ: observe_metapop(t, x,\n",
    "                                                rho            = θ[0, :],\n",
    "                                                N              = H[:, [t]],\n",
    "                                                num_tests      = tests[:, [t]],\n",
    "                                                model_settings = model_settings)\n",
    "\n",
    "# f0 model for the ifeakf            | initial_condition(c0, pop=2000, m=300)\n",
    "initial_guess_x0_gamma  = lambda θ, gamma:  init_metapop(\n",
    "                                                N0             = H[:, 0],\n",
    "                                                c0             = gamma, # importation rate\n",
    "                                                model_settings = model_settings)\n",
    "\n",
    "\n",
    "ρmin = 0.01 # test sensitivity minimum\n",
    "ρmax = 0.5  # test sensitivity maximum\n",
    "\n",
    "βmin = 0.001 # transmission rate minimum\n",
    "βmax = 0.5   # transmission rate maximum\n",
    "\n",
    "max_total_pop     = np.max(H.sum(axis=0))\n",
    "state_space_range = np.array([0, max_total_pop])\n",
    "parameters_range  = np.array([[ρmin, ρmax],\n",
    "                              [βmin, βmax]])\n",
    "\n",
    "σ_perturb         = np.array([(ρmax - ρmin)   / 4,\n",
    "                                (βmax - βmin) / 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amro2cute(amro):\n",
    "    if amro == 'ESCHERICHIA COLI':\n",
    "        return \"e_coli\"\n",
    "    elif amro == 'KLEBSIELLA PNEUMONIAE':\n",
    "        return \"k_pneumoniae\"\n",
    "    elif amro==\"PSEUDOMONAS AERUGINOSA\":\n",
    "        return \"p_aeruginosa\"\n",
    "    elif amro==\"METHICILLIN-SUSCEPTIBLE STAPHYLOCOCCUS AUREUS\":\n",
    "        return \"mssa\"\n",
    "    elif amro==\"METHICILLIN-RESISTANT STAPHYLOCOCCUS AUREUS\":\n",
    "        return \"mrsa\"\n",
    "    elif amro==\"STAPHYLOCOCCUS EPIDERMIDIS\":\n",
    "        return \"s_epidermidis\"\n",
    "    elif amro==\"ENTEROCOCCUS FAECALIS\":\n",
    "        return \"e_faecalis\"\n",
    "    elif amro==\"ENTEROCOCCUS FAECIUM\":\n",
    "        return \"e_faecium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaosdonkey06/anaconda3/envs/pompjax/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../pompjax/pompjax/\")\n",
    "\n",
    "from pyro.contrib.forecast import eval_crps\n",
    "from eval import calibration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_evals(samples, obs, beta, rho,  name_var=\"beta\"):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        samples: num_ensembles x num_times\n",
    "        obs:     time series observation\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with the continuos ranked probability score (crps) and the average calibration.\n",
    "    \"\"\"\n",
    "\n",
    "    cal_df = calibration.calibration(np.expand_dims(samples.T, 0), np.expand_dims(obs, 0), observation_index=0)\n",
    "    sc     = np.mean(np.abs(cal_df.quantiles.values-cal_df.proportion_inside.values))\n",
    "\n",
    "    df_response                      = pd.DataFrame(columns=['crps', 'calibration_score', name_var, \"rho\"])\n",
    "    df_response['crps']              = [eval_crps(samples, obs)]\n",
    "    df_response[\"calibration_score\"] = sc\n",
    "    df_response[name_var]            = [beta]\n",
    "    df_response['rho']               = [rho]\n",
    "    return df_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df               = pd.read_csv(os.path.join(data_db_dir, \"long_files_8_25_2021\", \"patient_movement_2022-Nov.csv\"), parse_dates=['date'])\n",
    "patient_df               = patient_df.drop_duplicates(['date','mrn'])\n",
    "patient_df[\"ward_total\"] = patient_df.apply(lambda x: x[\"ward\"]+\"-\"+x[\"building\"]+\"-\"+x[\"place\"], axis=1)\n",
    "\n",
    "ward2id                  = {w: i for i, w in enumerate(patient_df[\"ward_total\"].index.values)}\n",
    "\n",
    "duplicated_pos_tests = (patient_df[['encounter_id','organism_name']].duplicated() & ~patient_df['organism_name'].isnull())\n",
    "\n",
    "patient_df.loc[duplicated_pos_tests,'test']          = 0\n",
    "patient_df.loc[duplicated_pos_tests,'organism_name'] = np.nan\n",
    "\n",
    "wards       = patient_df.ward_total.unique()\n",
    "amro_search = ['ESCHERICHIA COLI', 'KLEBSIELLA PNEUMONIAE', 'PSEUDOMONAS AERUGINOSA', 'METHICILLIN-SUSCEPTIBLE STAPHYLOCOCCUS AUREUS',\n",
    "                'METHICILLIN-RESISTANT STAPHYLOCOCCUS AUREUS', 'STAPHYLOCOCCUS EPIDERMIDIS', 'ENTEROCOCCUS FAECALIS', 'ENTEROCOCCUS FAECIUM']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combination(arr1, arr2):\n",
    "    a = []\n",
    "    for a1 in arr1:\n",
    "        for a2 in arr2:\n",
    "            a.append([a1, a2])\n",
    "    return np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ρmin    = 1/100\n",
    "ρmax    = 20/100\n",
    "\n",
    "βmin    = 0.01\n",
    "βmax    = 0.5\n",
    "\n",
    "ρ_search = np.arange(ρmin, ρmax+1/100, 1/100)\n",
    "β_search = np.arange(βmin, βmax+0.01, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def grid_search(amro, gamma, amro_df, model_settings, previous_search=None):\n",
    "    amro_df  = amro_df[amro_df.amro==amro].groupby(\"date\").sum(numeric_only=True).reset_index()\n",
    "    obs_amro = amro_df.set_index(\"date\").resample(\"W-Sun\").sum()[\"num_positives\"].values[4:]\n",
    "\n",
    "    process_model    = lambda t, x, θ : process_model_gamma(t, x, θ, gamma=gamma)\n",
    "    initial_guess_x0 = lambda θ:  initial_guess_x0_gamma(θ, gamma=gamma)\n",
    "\n",
    "    ρstep   = 1/100\n",
    "    ρmin    = 0\n",
    "    ρmax    = 20/100\n",
    "\n",
    "    βstep   = 0.01\n",
    "    βmin    = 0\n",
    "    βmax    = 0.5\n",
    "\n",
    "    ρ_search  = np.arange(ρmin, ρmax + ρstep, ρstep)\n",
    "    β_search  = np.arange(βmin, βmax + βstep, βstep)\n",
    "\n",
    "\n",
    "    if previous_search is not None:\n",
    "        beta_done = previous_search[\"beta\"].unique()\n",
    "        rho_done  = previous_search[\"rho\"].unique()\n",
    "\n",
    "        ρ_search1 = np.setdiff1d(ρ_search, rho_done)\n",
    "        β_search1 = np.setdiff1d(β_search, beta_done)\n",
    "\n",
    "    psearch = generate_combination(ρ_search1, beta_done)\n",
    "    psearch = np.concatenate([psearch, generate_combination(rho_done, β_search1)])\n",
    "\n",
    "    metric_df = []\n",
    "    for idx_s, p in tqdm(enumerate(psearch), total=len(psearch)):\n",
    "        ρsim = p[0]\n",
    "        βsim = p[1]\n",
    "\n",
    "        θsim               = np.array([[ρsim], [βsim]]) * np.ones((2, model_settings[\"m\"]))\n",
    "        y_sim              = simulate_metapop_observations(process_model, observational_model, initial_guess_x0, θsim, model_settings)\n",
    "        observations       = np.sum(y_sim, axis=1)\n",
    "        sim_df             = pd.DataFrame(columns=[\"date\", \"ens_id\", \"values\", \"scenario\"])\n",
    "        sim_df[\"values\"]   = observations.flatten()\n",
    "        sim_df[\"date\"]     = flatten_list([ [date]*model_settings[\"m\"]  for date in  list(model_settings[\"dates\"])])\n",
    "        sim_df[\"ens_id\"]   = list(range(model_settings[\"m\"] )) * len(model_settings[\"dates\"])\n",
    "        sim_df[\"rho\"]      = ρsim\n",
    "\n",
    "        samples_t  = sim_df.set_index([\"date\", \"ens_id\", \"rho\"]).unstack([1, 2]).resample(\"W-Sun\").sum(numeric_only=True).stack().stack().reset_index()\n",
    "        samples_t  = pd.pivot(data=samples_t, index=\"date\", columns=\"ens_id\", values=\"values\").to_numpy()\n",
    "        samples_t  = samples_t[4:, :].T\n",
    "        samples_t  = torch.tensor(samples_t);  obs_t  = torch.tensor(list(obs_amro))\n",
    "        df_metrics = compute_evals(samples_t, obs_t, βsim, ρsim)\n",
    "        metric_df.append(df_metrics)\n",
    "    return pd.concat([pd.concat(metric_df).reset_index(drop=True), previous_search])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid search for  ESCHERICHIA COLI  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 329/329 [30:33<00:00,  5.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid search for  KLEBSIELLA PNEUMONIAE  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 243/329 [22:40<08:23,  5.86s/it]"
     ]
    }
   ],
   "source": [
    "amro_prev_df     = pd.read_csv(os.path.join(\"..\", \"data\", \"amro_prevalence.csv\"))\n",
    "amro_df = pd.read_csv( os.path.join(data_db_dir, \"long_files_8_25_2021\", \"amro_ward.csv\" ), parse_dates=[\"date\"])\n",
    "\n",
    "for amro in amro_search:\n",
    "    print(\"grid search for \", amro, \" ...\")\n",
    "    gamma          = amro_prev_df[amro_prev_df.amro==amro][\"prevalence_mean1\"].values[0]/100\n",
    "    prev_search_df = pd.read_csv(os.path.join(results2_dir, \"grid_search\", \"metapopulation\", f\"{amro2cute(amro)}.csv\") )\n",
    "    crps_amro_df   = grid_search(amro, gamma, amro_df, model_settings, previous_search=prev_search_df)\n",
    "    crps_amro_df.to_csv( os.path.join(results2_dir, \"grid_search\", \"metapopulation\", f\"{amro2cute(amro)}2.csv\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_amro_df             = pd.read_csv( os.path.join(results2_dir, \"grid_search\", \"metapopulation\", f\"{amro2cute(amro)}2.csv\") ).reset_index(drop=True)\n",
    "grid_amro_df             = grid_amro_df.drop_duplicates(subset=[\"beta\", \"rho\"]).drop(columns=[\"Unnamed: 0\", \"Unnamed: 0.1\"])\n",
    "grid_amro_df[\"beta_str\"] = grid_amro_df.beta.apply(lambda x: f\"{x:.5f}\")\n",
    "grid_amro_df[\"rho_str\"]  = grid_amro_df.rho.apply(lambda x: f\"{x:.5f}\")\n",
    "hm_crps_df               = grid_amro_df.pivot(index='beta', columns='rho', values='crps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "amro_search = ['ESCHERICHIA COLI', 'KLEBSIELLA PNEUMONIAE', 'PSEUDOMONAS AERUGINOSA', 'METHICILLIN-SUSCEPTIBLE STAPHYLOCOCCUS AUREUS',\n",
    "                'METHICILLIN-RESISTANT STAPHYLOCOCCUS AUREUS', 'STAPHYLOCOCCUS EPIDERMIDIS', 'ENTEROCOCCUS FAECALIS', 'ENTEROCOCCUS FAECIUM']\n",
    "\n",
    "fig, ax = plt.subplots(2, 4, figsize=(12, 5), sharey=True, sharex=True)\n",
    "\n",
    "for idx_axi, axi in enumerate(ax.flatten()):\n",
    "    amro         = amro_search[idx_axi]\n",
    "    grid_amro_df = pd.read_csv( os.path.join(results2_dir, \"grid_search\", \"metapopulation\", f\"{amro2cute(amro)}2.csv\") ).reset_index(drop=True)\n",
    "    grid_amro_df = grid_amro_df.drop_duplicates(subset=[\"beta\", \"rho\"]).drop(columns=[\"Unnamed: 0\", \"Unnamed: 0.1\"])\n",
    "    grid_amro_df = grid_amro_df[grid_amro_df.rho<0.3]\n",
    "\n",
    "    grid_amro_df[\"beta_str\"] = grid_amro_df.beta.apply(lambda x: f\"{x:.3f}\")\n",
    "    grid_amro_df[\"rho_str\"]  = grid_amro_df.rho.apply(lambda x: f\"{x:.3f}\")\n",
    "    hm_crps_df               = grid_amro_df.pivot(index='beta', columns='rho', values='crps')\n",
    "\n",
    "    sns.heatmap(ax=axi, data=np.log10(hm_crps_df), cmap='Reds')\n",
    "    axi.set_ylabel(None)\n",
    "    axi.set_xlabel(None)\n",
    "    axi.set_title(\". \".join(amro2cute(amro).split(\"_\")).capitalize())\n",
    "\n",
    "ax[0, 0].set_ylabel(r'$\\beta$')\n",
    "ax[1, 0].set_ylabel(r'$\\beta$')\n",
    "\n",
    "for i in range(4):\n",
    "    ax[1, i].set_xlabel(r'$\\rho$')\n",
    "\n",
    "fig.suptitle(r\"Metapopulation $\\log_{{10}}(CRPS)$ landscapes\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(12, 5), sharey=True, sharex=True)\n",
    "\n",
    "for idx_axi, axi in enumerate(ax.flatten()):\n",
    "    amro         = amro_search[idx_axi]\n",
    "\n",
    "    grid_amro_df = pd.read_csv( os.path.join(results2_dir, \"grid_search\", \"metapopulation\", f\"{amro2cute(amro)}2.csv\") ).reset_index(drop=True)\n",
    "    grid_amro_df = grid_amro_df.drop_duplicates(subset=[\"beta\", \"rho\"]).drop(columns=[\"Unnamed: 0\", \"Unnamed: 0.1\"])\n",
    "    grid_amro_df = grid_amro_df[grid_amro_df.rho<0.3]\n",
    "\n",
    "    grid_amro_df[\"beta_str\"] = grid_amro_df.beta.apply(lambda x: f\"{x:.2f}\")\n",
    "    grid_amro_df[\"rho_str\"]  = grid_amro_df.rho.apply(lambda x: f\"{x:.2f}\")\n",
    "    hm_cov_df    = grid_amro_df.pivot(index='beta', columns='rho', values='calibration_score')\n",
    "\n",
    "    sns.heatmap(ax=axi, data=hm_cov_df, cmap='Reds')\n",
    "    axi.set_ylabel(None)\n",
    "    axi.set_xlabel(None)\n",
    "    axi.set_title(\". \".join(amro2cute(amro).split(\"_\")).capitalize())\n",
    "\n",
    "ax[0, 0].set_ylabel(r'$\\beta$')\n",
    "ax[1, 0].set_ylabel(r'$\\beta$')\n",
    "\n",
    "for i in range(4):\n",
    "    ax[1, i].set_xlabel(r'$\\rho$')\n",
    "\n",
    "fig.suptitle(\"Metapopulation average calibration landscapes\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jeff: i think we want to have 10 synthetic test per importation, with the real importations.\n",
    "# me:   we could use the crps or E[calibration] to choose pairs beta, rho that fall in the 5% of lower calibration/crps ladnscape so we knew they minimize the hospital level observations.\n",
    "# jeff: sure\n",
    "# rami: I am not sure how do you calculate exactly credible intervals for the inference?\n",
    "# jeff: First try taking it straight from the ensemble posterior distribution of a single inference.\n",
    "#       It is not always great unless the inference is calibrated and can be sensitive to where in the inference you pull the solution (the end, 20 days prior).\n",
    "#       Run it many times and take a range of 95% of the spread? Alternatively take the mean estimates from multiple inferences (this will likely be too narrow).\n",
    "# rami: Or calculate profiles by fixing one parameter and fitting the other one, like Jaime suggested the other day.\n",
    "# jeff: I don’t think this makes sense.\n",
    "# rami: Is there a standard procedure for this with this type of inference?\n",
    "# jeff: If well calibrated use the posterior\n",
    "# rami: Also, why would the inference results come out different than the likelihood surfaces?\n",
    "# jeff: Bias in the inference estimation.\n",
    "# jeff: For a 2 parameter system, running LL is simple and is a brute force approach (like an MCMC).\n",
    "#       As the dimension of the system increases, iterated approaches offer advantages, and the EAKF in particular is not subject to the curse of dimensionality.\n",
    "# rami: And if that's the case mabe its better to work in this case with likelihood instead of EAKF?\n",
    "# jeff: For the 2-parameter system it might be. But look at the credible intervals per the above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "def return_score_cutoff(score, cut_off_prob=0.05):\n",
    "    freq, score = np.histogram(score, bins=100, density=True)\n",
    "    freq_cum    = np.cumsum(freq); freq_cum = freq_cum/freq_cum[-1]\n",
    "    score       = score[1:]\n",
    "    f_cum       = UnivariateSpline(score, freq_cum, s=0.001)\n",
    "    sc_range    = np.linspace(np.min(score), np.max(score), 1000)\n",
    "    score_cut   = sc_range[np.argmin(np.abs(f_cum(sc_range) * 100 - cut_off_prob*100))]\n",
    "    return score_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from misc import amro2cute\n",
    "import seaborn as sns\n",
    "\n",
    "amro_search  = ['ESCHERICHIA COLI', 'KLEBSIELLA PNEUMONIAE',  'PSEUDOMONAS AERUGINOSA',\n",
    "                'METHICILLIN-SUSCEPTIBLE STAPHYLOCOCCUS AUREUS',\n",
    "                \"STAPHYLOCOCCUS EPIDERMIDIS\", 'ENTEROCOCCUS FAECALIS', 'ENTEROCOCCUS FAECIUM']\n",
    "\n",
    "cut_off_prob = 5/100\n",
    "\n",
    "for amro in amro_search:\n",
    "\n",
    "    grid_amro_df             = pd.read_csv( os.path.join(results2_dir, \"grid_search\", \"metapopulation\", f\"{amro2cute(amro)}2.csv\") )\n",
    "    grid_amro_df = grid_amro_df.drop_duplicates(subset=[\"beta\", \"rho\"]).drop(columns=[\"Unnamed: 0\", \"Unnamed: 0.1\"])\n",
    "    grid_amro_df = grid_amro_df[grid_amro_df.rho<0.3]\n",
    "\n",
    "    grid_amro_df[\"beta_str\"] = grid_amro_df.beta.apply(lambda x: f\"{x:.2f}\")\n",
    "    grid_amro_df[\"rho_str\"]  = grid_amro_df.rho.apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "    hm_amro_df   = grid_amro_df.pivot(index='beta_str', columns='rho_str', values='calibration_score')\n",
    "    freq, score = np.histogram(grid_amro_df.calibration_score, bins=100, density=True)\n",
    "    freq_cum    = np.cumsum(freq); freq_cum = freq_cum/freq_cum[-1]\n",
    "    score       = score[1:]\n",
    "    f_cum       = UnivariateSpline(score, freq_cum, s=0.001)\n",
    "    sc_range    = np.linspace(np.min(score), np.max(score), 1000)\n",
    "    score_cut   = sc_range[np.argmin(np.abs(f_cum(sc_range) * 100 - cut_off_prob*100))]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18.2, 5.5))\n",
    "\n",
    "    ax[1].bar(score, freq, align=\"center\", width=np.min(np.diff(score))*0.9, facecolor=\"lightblue\", edgecolor=\"k\", lw=0.3)\n",
    "    ax[1].set_xlabel(\"Calibration score\")\n",
    "\n",
    "    ax[1].spines['right'].set_visible(False)\n",
    "    ax[1].spines['left'].set_visible(False)\n",
    "    ax[1].spines['top'].set_visible(False)\n",
    "    ax[1].grid(axis='y', alpha=0.5)\n",
    "    ax[1].set_yscale('log')\n",
    "\n",
    "    ax[2].bar(score, freq_cum, align=\"center\", width=np.min(np.diff(score))*0.9, facecolor=\"lightblue\", edgecolor=\"k\", lw=0.3, label=\"Cum. freq.\")\n",
    "    ax[2].axhline(y=cut_off_prob,     color='r', linestyle='--', label=f\"y={int(cut_off_prob*100)}%\")\n",
    "    ax[2].axvline(x=score_cut, color='r', linestyle='--', label=\"calibration cut off: x={:.3f}\".format(score_cut))\n",
    "    ax[2].plot(sc_range, f_cum(sc_range), color='salmon', linestyle='--', lw=5, label=\"Spline\")\n",
    "\n",
    "    ax[2].set_ylabel(\"Cumulative probability\")\n",
    "    ax[2].set_xlabel(\"Calibration score\")\n",
    "\n",
    "    ax[2].spines['right'].set_visible(False)\n",
    "    ax[2].spines['left'].set_visible(False)\n",
    "    ax[2].spines['top'].set_visible(False)\n",
    "    ax[2].grid(axis='y', alpha=0.5)\n",
    "    ax[2].legend()\n",
    "\n",
    "    sns.heatmap(ax=ax[0], data=hm_amro_df, edgecolor=\"red\", cmap='Blues', cbar_kws={'label': 'Calibration score'}, fmt=\".2f\", cbar=True, vmin=0.2, vmax=0.6)\n",
    "    hm_amro_df[hm_amro_df > score_cut] = np.nan\n",
    "\n",
    "    sns.heatmap(ax=ax[0], data=hm_amro_df, edgecolor=\"red\", cmap='cool',\n",
    "                    cbar_kws={'label': 'Calibration score'}, fmt=\".2f\", cbar=False, vmin=0.2, vmax=0.6)\n",
    "\n",
    "    ax[0].set_ylabel(r\"$\\beta$\")\n",
    "    ax[0].set_xlabel(r\"$\\rho$\")\n",
    "\n",
    "    fig.suptitle(\"{}\".format(\". \".join(amro2cute(amro).split(\"_\")).capitalize()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from misc import amro2cute\n",
    "import seaborn as sns\n",
    "\n",
    "amro_search  = ['ESCHERICHIA COLI', 'KLEBSIELLA PNEUMONIAE', 'PSEUDOMONAS AERUGINOSA',\n",
    "                'METHICILLIN-SUSCEPTIBLE STAPHYLOCOCCUS AUREUS', \"STAPHYLOCOCCUS EPIDERMIDIS\",\n",
    "                'ENTEROCOCCUS FAECALIS', 'ENTEROCOCCUS FAECIUM']\n",
    "\n",
    "cut_off_prob = 10 / 100\n",
    "\n",
    "for amro in amro_search:\n",
    "\n",
    "    grid_amro_df             = pd.read_csv( os.path.join(results2_dir, \"grid_search\", \"metapopulation\", f\"{amro2cute(amro)}2.csv\") )\n",
    "    grid_amro_df = grid_amro_df.drop_duplicates(subset=[\"beta\", \"rho\"]).drop(columns=[\"Unnamed: 0\", \"Unnamed: 0.1\"])\n",
    "    grid_amro_df = grid_amro_df[grid_amro_df.rho<0.3]\n",
    "\n",
    "    grid_amro_df[\"beta_str\"] = grid_amro_df.beta.apply(lambda x: f\"{x:.2f}\")\n",
    "    grid_amro_df[\"rho_str\"]  = grid_amro_df.rho.apply(lambda x: f\"{x:.2f}\")\n",
    "    grid_amro_df[\"crps\"]     = np.log10(grid_amro_df.crps)\n",
    "\n",
    "    hm_amro_df   = grid_amro_df.pivot(index='beta', columns='rho', values='crps')\n",
    "\n",
    "    freq, score = np.histogram(grid_amro_df.crps, bins=100, density=True)\n",
    "    freq_cum    = np.cumsum(freq); freq_cum = freq_cum/freq_cum[-1]\n",
    "    score       = score[1:]\n",
    "    f_cum       = UnivariateSpline(score, freq_cum, s=0.001)\n",
    "    sc_range    = np.linspace(np.min(score), np.max(score), 1000)\n",
    "    score_cut   = sc_range[np.argmin(np.abs(f_cum(sc_range) * 100 - cut_off_prob*100))]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18.2, 5.5))\n",
    "\n",
    "    ax[1].bar(score, freq, align=\"center\", width=np.min(np.diff(score))*0.9, facecolor=\"lightblue\", edgecolor=\"k\", lw=0.3)\n",
    "    ax[1].set_xlabel(r\"$\\log_{{10}}$(CRPS)\")\n",
    "\n",
    "    ax[1].spines['right'].set_visible(False)\n",
    "    ax[1].spines['left'].set_visible(False)\n",
    "    ax[1].spines['top'].set_visible(False)\n",
    "    ax[1].grid(axis='y', alpha=0.5)\n",
    "    ax[1].set_yscale('log')\n",
    "\n",
    "    ax[2].bar(score, freq_cum, align=\"center\", width=np.min(np.diff(score))*0.9, facecolor=\"lightblue\", edgecolor=\"k\", lw=0.3, label=\"Cum. freq.\")\n",
    "    ax[2].axhline(y=cut_off_prob,     color='r', linestyle='--', label=f\"y={int(cut_off_prob*100)}%\")\n",
    "    ax[2].axvline(x=score_cut, color='r', linestyle='--', label=\"calibration cut off: x={:.3f}\".format(score_cut))\n",
    "    ax[2].plot(sc_range, f_cum(sc_range), color='salmon', linestyle='--', lw=5, label=\"Spline\")\n",
    "\n",
    "    ax[2].set_ylabel(\"Cumulative probability\")\n",
    "    ax[2].set_xlabel(r\"$\\log_{{10}}$(CRPS)\")\n",
    "\n",
    "    ax[2].spines['right'].set_visible(False)\n",
    "    ax[2].spines['left'].set_visible(False)\n",
    "    ax[2].spines['top'].set_visible(False)\n",
    "    ax[2].grid(axis='y', alpha=0.5)\n",
    "    ax[2].legend()\n",
    "\n",
    "    sns.heatmap(ax=ax[0], data=hm_amro_df, edgecolor=\"red\", cmap='Reds', cbar_kws={'label': r\"$\\log_{{10}}$(CRPS)\"}, fmt=\".2f\", cbar=True, vmin=0.5, vmax=2)\n",
    "    hm_amro_df[hm_amro_df > score_cut] = np.nan\n",
    "\n",
    "    sns.heatmap(ax=ax[0], data=hm_amro_df, edgecolor=\"red\", cmap='cool',\n",
    "                    cbar_kws={'label': 'Calibration score'}, fmt=\".2f\", cbar=False, vmin=0.5, vmax=2)\n",
    "\n",
    "    ax[0].set_ylabel(r\"$\\beta$\")\n",
    "    ax[0].set_xlabel(r\"$\\rho$\")\n",
    "\n",
    "    fig.suptitle(\"{}\".format(\". \".join(amro2cute(amro).split(\"_\")).capitalize()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pompjax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e23b8103492689b0d9748b47018d3861a561878402e3a1e7a565e9dcb2ea3867"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
